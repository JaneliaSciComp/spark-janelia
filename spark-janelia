#!/usr/local/python-2.7.6/bin/python2.7

import os
import glob
import sys
import argparse
import subprocess
import xml.etree.ElementTree as ET

parser = argparse.ArgumentParser(description="launch and manage spark cluster jobs")

parser.add_argument("task", choices=('launch', 'login', 'destroy', 'start', 'start-scala', 'submit'))
parser.add_argument("-n", "--nnodes", type=int, default=2, required=False)
parser.add_argument("-i", "--ipython", action="store_true")
parser.add_argument("-v", "--version", choices=("1", "2", "3"), default="1", required=False)
parser.add_argument("-j", "--jobid", type=int, default=None, required=False)

args = parser.parse_args()

SPARKVERSIONS = {
    '1': '/usr/local/spark-current',
    '2': '/usr/local/spark-master',
    '3': '/usr/local/spark-test'
}

SPARKVERSIONSSHORT = {
    '1': 'spark',
    '2': 'spark2',
    '3': 'spark3'
}

version = SPARKVERSIONS[args.version]
version_short = SPARKVERSIONSSHORT[args.version]

if args.nnodes == 1:
    raise Exception('Cannot start a Spark cluster with only 1 node, please request 2 or more.')

if args.task == 'launch':
    if not os.path.exists(os.path.expanduser('~/sparklogs')):
        os.mkdir(os.path.expanduser('~/sparklogs'))

    output = subprocess.check_output(['qsub', '-jc', version_short, '-pe', version_short, str(args.nnodes), '-q', 'hadoop2', '-j', 'y', '-o', os.path.expanduser('~/sparklogs/'), '/sge/8.1.4/examples/jobs/sleeper.sh', '86400'])
    print('\n')
    print('Spark job submitted with ' + str(args.nnodes) + ' nodes')
    print('\n')

elif args.task == 'login':
    status = subprocess.check_output(["qstat", "-xml"])
    qtree = ET.fromstring(status)[0]
    jtree = ET.fromstring(status)[1]
    jobs = qtree.findall('job_list')
    if len(jobs) == 0:
        print('\n')
        print >> sys.stderr, "No running jobs found, keep waiting for jobs to launch"
        print('\n')
    else:
        address = None
        for job in jobs:
            jobclass = job.find('jclass_name').text
            state = job.find('state').text
            jobid = int(job.find('JB_job_number').text)
            if (jobclass == version_short + ".default") and (state == 'r'):
                if args.jobid:
                    if jobid == args.jobid:
                        address = job.find('queue_name').text.split('@')[1]
                else:
                    address = job.find('queue_name').text.split('@')[1]
        if address:
            filename = os.path.expanduser('~/spark-master')
            if os.path.exists(filename):
                os.remove(filename)
            os.system("echo 'spark://'" + address + ":7077 >> " + os.path.expanduser("~") + "/spark-master")
            subprocess.check_call(['ssh', address])
        else:
            print('\n')
            print >> sys.stderr, "No Spark job found, check status with qstat, or try a different jobid?"
            print('\n')

elif args.task == 'destroy':
    status = subprocess.check_output(["qstat", "-xml"])
    qtree = ET.fromstring(status)[0]
    jtree = ET.fromstring(status)[1]
    jobs = qtree.findall('job_list')
    if len(jobs) == 0:
        print('\n')
        print >> sys.stderr, "No running jobs found"
        print('\n')
    else:
        deleted = 0
        for job in jobs:
            jobclass = job.find('jclass_name').text
            state = job.find('state').text
            jobid = job.find('JB_job_number').text
            if (jobclass == version_short + ".default") and (state == 'r'):
                if args.jobid:
                    if int(jobid) == args.jobid:
                        output = subprocess.check_output(['qdel', jobid])
                        deleted += 1
                else:
                    output = subprocess.check_output(['qdel', jobid])
                    deleted += 1
        if deleted == 0:
            print('\n')
            print >> sys.stderr, "No Spark jobs deleted, try a different jobid?"
            print('\n')
        else:
            print('\n')
            print('Spark jobs successfully deleted')
            print('\n')

elif args.task == 'start':

    if args.ipython is True:
       os.environ['IPYTHON_OPTS'] = "notebook --profile=nbserver"

    os.environ['SPARK_HOME'] = version

    if os.getenv('PATH') is None:
        os.environ['PATH'] = ""

    if os.getenv('IPYTHON') is None:
       os.environ['IPYTHON'] = "1"

    os.environ['PATH'] = os.environ['PATH'] + ":" + "/usr/local/python-2.7.6/bin"
    f = open(os.path.expanduser("~") + '/spark-master', 'r')
    os.environ['MASTER'] = f.readline().replace('\n','')
    os.system(version + '/bin/pyspark')

elif args.task == 'start-scala':

    os.environ['SPARK_HOME'] = version

    if os.getenv('PATH') is None:
        os.environ['PATH'] = ""

    os.environ['PATH'] = os.environ['PATH'] + ":" + "/usr/local/python-2.7.6/bin"
    f = open(os.path.expanduser("~") + '/spark-master', 'r')
    os.environ['MASTER'] = f.readline().replace('\n','')
    os.system(version + '/bin/spark-shell')


